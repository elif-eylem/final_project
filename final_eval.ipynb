{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108b351a-f42e-4400-9d83-9663578e71c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dccstor/bmfm-targets1/users/eyigoz/openai/notebooks/Final'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602a7431-19e7-4069-8160-176ec0df7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"  # shows all output, not just the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce985d6-ce1f-4075-990e-19583d86a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79e1d8e-85a8-47bf-b3a9-d71f92996645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6df5569-c1f7-4fdb-87a3-0f3f1b6646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scifact = False\n",
    "if scifact:\n",
    "    data_dir = '/dccstor/bmfm-targets1/users/eyigoz/openai/notebooks/Final/scifact/'\n",
    "    train_df = pd.read_json(data_dir+'claims_train.jsonl',lines=True)\n",
    "    dev_df = pd.read_json(data_dir+'claims_dev.jsonl',lines=True)\n",
    "    test_df = pd.read_json(data_dir+'claims_test.jsonl',lines=True)\n",
    "    out_dir = '/dccstor/bmfm-targets1/users/eyigoz/openai/notebooks/Final/scifact_split_csvs/'\n",
    "    def get_labels(evidence):\n",
    "        labels = []\n",
    "        for val in evidence.values():\n",
    "            if len(val) > 0:\n",
    "                for evi in val:\n",
    "                    labels.append(evi['label'])\n",
    "        labels = set(labels)\n",
    "        if len(labels) > 0:\n",
    "            return labels.pop()\n",
    "            \n",
    "        return None\n",
    "            \n",
    "    train_df['label'] = train_df.evidence.apply(lambda x: get_labels(x))\n",
    "    \n",
    "    train_df['label'] = train_df['label'].fillna('Not Enough Evidence')\n",
    "    \n",
    "    train_df.label.value_counts()\n",
    "    eval_df = train_df\n",
    "\n",
    "\n",
    "else:\n",
    "    train_df = pd.read_json('averitec_data/train.json')\n",
    "    dev_df = pd.read_json('averitec_data/dev.json')\n",
    "    test_df = pd.read_json('averitec_data/test.json')\n",
    "    # train_df = pd.read_csv('train_df.v1.csv')\n",
    "    # dev_df = pd.read_csv('dev_df.v1.csv')\n",
    "    # test_df = pd.read_csv('test_df.v1.csv')\n",
    "    out_dir = '/dccstor/bmfm-targets1/users/eyigoz/openai/notebooks/Final/averitec_split_csvs/'\n",
    "    eval_df = dev_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ec51a-0196-41c0-8e16-64227e25facb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1658ffa9-31da-4905-853c-88b1e873d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35aa1e-23ab-461b-ae43-07a7755698d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6b712f-41b7-42fe-b096-d6fd2cb7b83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_files = [x for x in os.listdir(out_dir) if x.endswith(\".pkl\") and \"dev\" in x ]\n",
    "len(pkl_files)\n",
    "# pkl_f = pkl_files[0] \n",
    "# pkl_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e440d62d-0c4e-42be-87e6-4678f151a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_len = []\n",
    "all_states = []\n",
    "for pkl_f in pkl_files:\n",
    "    states = pickle.load(open(out_dir+pkl_f,'rb'))\n",
    "    all_states.append(states)\n",
    "    summary_len.extend(states.apply(lambda x: len(x['summaries'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbeb375d-2b19-4627-8636-0d9fa1307a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=484, minmax=(25, 565176), mean=58482.39462809917, variance=3027796487.9660997, skewness=4.431800308839087, kurtosis=27.53847616669141)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(summary_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d21c8fd2-c249-45c1-ae47-3e772911b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.concat(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20fad22c-3ac0-49c4-a4f6-6e95d328dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41be33fa-21c6-4e6e-8729-3c452490f44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'text': 'In a letter to Steve Jobs, Sean Conn...\n",
       "1    {'text': 'Trump Administration claimed songwri...\n",
       "2    {'text': 'Due to Imran Khan's criticism of Mac...\n",
       "3    {'text': 'UNESCO declared Nadar community as t...\n",
       "4    {'text': 'Republican Matt Gaetz was part of a ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3c406d2-4565-4b0b-942a-0fadc50430c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_df = pd.concat([eval_df.loc[states.index], states],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cf6ee97-4423-44d3-9ec9-ac235d25239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.rename({0:'states'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ddda75-f917-422e-9d76-008b97327a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73c12ffe-cc4a-4ef6-8173-2fc3452d74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_results(raw_response,key):\n",
    "#     clean_json = raw_response.strip('`').replace(\"json\\n\", \"\", 1).strip()\n",
    "    \n",
    "#     # Step 2: Parse to dict\n",
    "#     try:\n",
    "#         parsed = json.loads(clean_json)\n",
    "#         return parsed[key]\n",
    "#     except Exception as e:\n",
    "#         print (e)\n",
    "#         print (clean_json)\n",
    "#     # print(parsed[\"result\"])       # supports\n",
    "#     # print(parsed[\"explanation\"])  # full explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87a2ce88-bbc2-4b6d-bcba-fcc1bd86e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(raw_response,keywords):\n",
    "    clean_json = raw_response.strip('`').replace(\"json\\n\", \"\", 1).strip()\n",
    "    cleaned_json_str = re.sub(r'\\t', r'\\\\t', clean_json)\n",
    "    \n",
    "    # Step 2: Parse to dict\n",
    "    try:\n",
    "        parsed = json.loads(cleaned_json_str)\n",
    "        keys = parsed.keys()\n",
    "        for key in keys:\n",
    "            all_in = True\n",
    "            for keyword in keywords:\n",
    "                if not keyword in key:\n",
    "                    all_in = False\n",
    "                \n",
    "            if all_in:\n",
    "                old_key = key\n",
    "        #print (\"--\",old_key)        \n",
    "        #parsed[replace_key] = parsed[old_key]  \n",
    "        return parsed[old_key]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print (e)\n",
    "        #print (clean_json)\n",
    "    # print(parsed[\"result\"])       # supports\n",
    "    # print(parsed[\"explanation\"])  # full explanation\n",
    "\n",
    "eval_df['evidence'] = eval_df.states.apply\\\n",
    "    (lambda x: parse_results(x['qa_pairs'],['question','answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc0f9017-965f-4e19-be63-b3e1036d58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['search_query'] = eval_df.states.apply\\\n",
    "    (lambda x: x['search_query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e2a1db8-1ba8-4d59-8fa5-7eaeff8981b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['topic_questions'] = eval_df.states.apply\\\n",
    "    (lambda x: x['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b587ef62-833c-4e1c-8523-835b9ba8ea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'required_reannotation', 'label', 'justification',\n",
       "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
       "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
       "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
       "       'states', 'evidence', 'search_query', 'topic_questions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af3df8-2f40-40ea-abc4-e4b61b5c4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "['claim', 'evidence', 'search_query', 'topic_questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a780556-0db6-43e0-a92a-0d3945b3ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: The Trump RNC Acceptance Speech Was The First Time The White House Was Used For 'Purely Political Campaign Event'\n",
      "Label: Refuted\n",
      "Search query:  Trump RNC acceptance speech, White House political event, first time White House used for campaign, presidential campaign events at White House\n",
      "Generated questons:\n",
      " 1. Was the Trump RNC Acceptance Speech held at the White House?\n",
      "2. Was the Trump RNC Acceptance Speech a purely political campaign event?\n",
      "3. Had the White House been used for a purely political campaign event prior to the Trump RNC Acceptance Speech?\n",
      "4. Was the Trump RNC Acceptance Speech the first time the White House was used for a purely political campaign event?\n",
      "Here are the individual verifiable questions based on the claim:\n",
      "{'question': 'Was the Trump RNC Acceptance Speech held at the White House?', 'answer': 'President Donald Trump delivered his acceptance speech for the Republican National Convention from the South Lawn of the White House on August 27, 2020.'}\n",
      "\n",
      "{'question': 'Was the Trump RNC Acceptance Speech a purely political campaign event?', 'answer': \"The speech was a political event, with Trump using the White House backdrop to promote his campaign, displaying 'Trump Pence' in white block letters and his 2016 campaign slogan 'Make America Great Again' on jumbotrons.\"}\n",
      "\n",
      "{'question': 'Had the White House been used for a purely political campaign event prior to the Trump RNC Acceptance Speech?', 'answer': \"While past presidents have used the White House for political purposes, Trump's approach is more overt and brazen, and some critics argue that holding the speech at the White House may still violate the spirit of the law.\"}\n",
      "\n",
      "{'question': 'Was the Trump RNC Acceptance Speech the first time the White House was used for a purely political campaign event?', 'answer': \"While past presidents have used the White House for political purposes, Trump's approach is more overt and brazen, and some critics argue that holding the speech at the White House may still violate the spirit of the law.\"}\n"
     ]
    }
   ],
   "source": [
    "for ind, row in eval_df.sample(1).iterrows():\n",
    "    print(\"Claim:\" , row['claim'])\n",
    "    print(\"Label:\", row['label'])\n",
    "\n",
    "    print(\"Search query:\", row['search_query'])\n",
    "    print(\"Generated questons:\\n\", \"\\n\".join(sorted(set(row['topic_questions']))))\n",
    "    print(\"\\n\\n\".join([str(qa) for qa in row['evidence']]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f25c4ae8-86fd-4f98-85b9-84e42b13a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df['evidence'] = eval_df.states.apply\\\n",
    "#    (lambda x: parse_results(x['qa_pairs'],'question_answer_pairs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d879230-8c0f-499b-a7db-007c73eae982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evidence\n",
       "False    223\n",
       "True       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['evidence'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64a5da71-edaa-4479-9dee-a4cab07625b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df.iloc[0].states['verifier_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d27e7ca1-b647-4a70-a891-bb5eafc9cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['verifier_result_parsed'] = eval_df.states.apply\\\n",
    "    (lambda x: parse_results(x['verifier_result'],['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b6b66bb-b42c-411b-9c72-bdf076151743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df['verifier_result_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f7906cc-ecb3-4c7d-9b61-c5a3ad1426af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verifier_result_parsed\n",
       "False    224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['verifier_result_parsed'] .isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04ebef9b-e406-442f-a51d-6091d55281a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verifier_result_parsed\n",
       "refutes                 116\n",
       "supports                 56\n",
       "not_enough_evidence      44\n",
       "conflicting_evidence      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['verifier_result_parsed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275c1e0-8f0f-4606-8c25-78cff613fda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab6b4f10-0ce2-443a-b352-610bc2cbad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scifact:\n",
    "    label_map = {'not_enough_evidence':'Not Enough Evidence',\\\n",
    "             'Not Enough Evidence':'Not Enough Evidence',\\\n",
    "            'supports':'Supported',\\\n",
    "             'SUPPORT':'Supported',\\\n",
    "             'Supported':'Supported',\\\n",
    "             'CONTRADICT':'Refuted',\\\n",
    "            'refutes':'Refuted',\\\n",
    "             'Refuted':'Refuted',\\\n",
    "            'conflicting_evidence':'Not Enough Evidence',\\\n",
    "            'Conflicting Evidence/Cherrypicking':'Conflicting Evidence/Cherrypicking'}\n",
    "else:\n",
    "    label_map = {'not_enough_evidence':'Not Enough Evidence',\\\n",
    "             'Not Enough Evidence':'Not Enough Evidence',\\\n",
    "            'supports':'Supported',\\\n",
    "             'SUPPORT':'Supported',\\\n",
    "             'Supported':'Supported',\\\n",
    "             'CONTRADICT':'Refuted',\\\n",
    "            'refutes':'Refuted',\\\n",
    "             'Refuted':'Refuted',\\\n",
    "            'conflicting_evidence':'Conflicting Evidence/Cherrypicking',\\\n",
    "            'Conflicting Evidence/Cherrypicking':'Conflicting Evidence/Cherrypicking'}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb127122-45ff-4de7-8099-998e64f56bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.dropna(subset=['verifier_result_parsed'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bd97c93-bd45-4a49-be39-9496a7e9031b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'required_reannotation', 'label', 'justification',\n",
       "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
       "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
       "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
       "       'states', 'evidence', 'search_query', 'topic_questions',\n",
       "       'verifier_result_parsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9c73042-f684-4c83-a026-d045c4a83a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['orginal_label'] = eval_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7077677b-6e6c-44ee-9988-4014e2151e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['pred_label'] = eval_df['verifier_result_parsed'].apply(lambda x: label_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "731bf569-30cf-4a76-b11b-ffa7d4b1ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['label'] = eval_df['label'].apply(lambda x: label_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11bd006d-7ca7-4ac0-b769-fe15fc25496d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Refuted                               121\n",
       "Supported                              67\n",
       "Not Enough Evidence                    20\n",
       "Conflicting Evidence/Cherrypicking     16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f3701c8-8163-4228-914f-58cd138266df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df['verifier_result_parsed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3ea4231-ec73-4873-b10f-0c89139059c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_label\n",
       "Refuted                               116\n",
       "Supported                              56\n",
       "Not Enough Evidence                    44\n",
       "Conflicting Evidence/Cherrypicking      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['pred_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "beea33fd-ecf6-4ed1-84b3-f6d46950fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df['evidence'].dropna().apply(lambda x: x[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c458fb1a-02a5-4fd6-8cc8-b402abbe575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>In a letter to Steve Jobs, Sean Connery refuse...</td>\n",
       "      <td>Trump Administration claimed songwriter Billie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>required_reannotation</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Refuted</td>\n",
       "      <td>Refuted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>justification</th>\n",
       "      <td>The answer and sources show that the claim was...</td>\n",
       "      <td>Seems that the Wzshington post accused the sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim_date</th>\n",
       "      <td>31-10-2020</td>\n",
       "      <td>31-10-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_claim_url</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fact_checking_article</th>\n",
       "      <td>https://web.archive.org/web/20201130144023/htt...</td>\n",
       "      <td>https://web.archive.org/web/20201103001419/htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporting_source</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>Instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_ISO_code</th>\n",
       "      <td>None</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim_types</th>\n",
       "      <td>[Event/Property Claim]</td>\n",
       "      <td>[Position Statement, Event/Property Claim]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fact_checking_strategies</th>\n",
       "      <td>[Written Evidence]</td>\n",
       "      <td>[Written Evidence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questions</th>\n",
       "      <td>[{'question': 'Where was the claim first publi...</td>\n",
       "      <td>[{'question': 'Has the Trump administration vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cached_original_claim_url</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>states</th>\n",
       "      <td>{'text': 'In a letter to Steve Jobs, Sean Conn...</td>\n",
       "      <td>{'text': 'Trump Administration claimed songwri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evidence</th>\n",
       "      <td>[{'question': 'Did Sean Connery write a letter...</td>\n",
       "      <td>[{'question': 'Were documents related to this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_query</th>\n",
       "      <td>Sean Connery, Steve Jobs, Apple commercial, r...</td>\n",
       "      <td>Trump administration, Billie Eilish, leaked d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_questions</th>\n",
       "      <td>[Here are the individual verifiable questions ...</td>\n",
       "      <td>[Here are the individual verifiable questions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verifier_result_parsed</th>\n",
       "      <td>refutes</td>\n",
       "      <td>supports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orginal_label</th>\n",
       "      <td>Refuted</td>\n",
       "      <td>Refuted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_label</th>\n",
       "      <td>Refuted</td>\n",
       "      <td>Supported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           0  \\\n",
       "claim                      In a letter to Steve Jobs, Sean Connery refuse...   \n",
       "required_reannotation                                                  False   \n",
       "label                                                                Refuted   \n",
       "justification              The answer and sources show that the claim was...   \n",
       "claim_date                                                        31-10-2020   \n",
       "speaker                                                                 None   \n",
       "original_claim_url                                                      None   \n",
       "fact_checking_article      https://web.archive.org/web/20201130144023/htt...   \n",
       "reporting_source                                                    Facebook   \n",
       "location_ISO_code                                                       None   \n",
       "claim_types                                           [Event/Property Claim]   \n",
       "fact_checking_strategies                                  [Written Evidence]   \n",
       "questions                  [{'question': 'Where was the claim first publi...   \n",
       "cached_original_claim_url                                               None   \n",
       "states                     {'text': 'In a letter to Steve Jobs, Sean Conn...   \n",
       "evidence                   [{'question': 'Did Sean Connery write a letter...   \n",
       "search_query                Sean Connery, Steve Jobs, Apple commercial, r...   \n",
       "topic_questions            [Here are the individual verifiable questions ...   \n",
       "verifier_result_parsed                                               refutes   \n",
       "orginal_label                                                        Refuted   \n",
       "pred_label                                                           Refuted   \n",
       "\n",
       "                                                                           1  \n",
       "claim                      Trump Administration claimed songwriter Billie...  \n",
       "required_reannotation                                                  False  \n",
       "label                                                                Refuted  \n",
       "justification              Seems that the Wzshington post accused the sin...  \n",
       "claim_date                                                        31-10-2020  \n",
       "speaker                                                                 None  \n",
       "original_claim_url                                                      None  \n",
       "fact_checking_article      https://web.archive.org/web/20201103001419/htt...  \n",
       "reporting_source                                                   Instagram  \n",
       "location_ISO_code                                                         US  \n",
       "claim_types                       [Position Statement, Event/Property Claim]  \n",
       "fact_checking_strategies                                  [Written Evidence]  \n",
       "questions                  [{'question': 'Has the Trump administration vo...  \n",
       "cached_original_claim_url                                               None  \n",
       "states                     {'text': 'Trump Administration claimed songwri...  \n",
       "evidence                   [{'question': 'Were documents related to this ...  \n",
       "search_query                Trump administration, Billie Eilish, leaked d...  \n",
       "topic_questions            [Here are the individual verifiable questions ...  \n",
       "verifier_result_parsed                                              supports  \n",
       "orginal_label                                                        Refuted  \n",
       "pred_label                                                         Supported  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "222c85e4-44ea-4b22-94f5-4ca851b16ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_df.evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27fd0f-dbf6-47b8-a1e3-0cfbdcf3cdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb379030-549d-40b6-b7ad-2f5a9e3d05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03ec5829-0785-4587-89d7-2ead3b77598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy supported'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7901785714285714"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'f1 supported'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6178861788617886"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy refuted'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6919642857142857"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'f1_refuted'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7088607594936709"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy not enough evidence'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'f1 not enough evidencet'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy_score(eval_df['pred_label'],eval_df.label)\n",
    "\n",
    "\"accuracy supported\"\n",
    "accuracy_score(eval_df['pred_label'].apply(lambda x: x == 'Supported'),\\\n",
    "               eval_df.label.apply(lambda x: x == 'Supported'))\n",
    "\"f1 supported\"\n",
    "\n",
    "f1_score(eval_df['pred_label'].apply(lambda x: x == 'Supported'),\\\n",
    "               eval_df.label.apply(lambda x: x == 'Supported'))\n",
    "\"accuracy refuted\"\n",
    "accuracy_score(eval_df['pred_label'].apply(lambda x: x == 'Refuted'),\\\n",
    "               eval_df.label.apply(lambda x: x == 'Refuted'))\n",
    "\"f1_refuted\"\n",
    "f1_score(eval_df['pred_label'].apply(lambda x: x == 'Refuted'),\\\n",
    "               eval_df.label.apply(lambda x: x == 'Refuted'))\n",
    "\n",
    "\"Accuracy not enough evidence\"\n",
    "\n",
    "accuracy_score(eval_df['pred_label'].apply(lambda x: x == 'Not Enough Evidence'),\\\n",
    "               eval_df.label.apply(lambda x: x == 'Not Enough Evidence'))\n",
    "\n",
    "\"f1 not enough evidencet\"\n",
    "f1_score(eval_df['pred_label'].apply(lambda x: x == 'Not Enough Evidence'),\\\n",
    "               eval_df.label.apply(lambda x: x == 'Not Enough Evidence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f07d3d64-3d6e-4d36-a426-37bf925baf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Refuted                               0.540179\n",
       "Supported                             0.299107\n",
       "Not Enough Evidence                   0.089286\n",
       "Conflicting Evidence/Cherrypicking    0.071429\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.label.value_counts() / eval_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0eda0fc3-51b6-4926-892f-7d7bd549c6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_label\n",
       "Refuted                               0.517857\n",
       "Supported                             0.250000\n",
       "Not Enough Evidence                   0.196429\n",
       "Conflicting Evidence/Cherrypicking    0.035714\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.pred_label.value_counts() / eval_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a85fc-9627-4aac-ae33-d5c5daa05d8b",
   "metadata": {},
   "source": [
    "# Create Submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b34a1d1-b630-452f-91c8-25913c854cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = eval_df[eval_df['evidence'].apply(lambda x: x != [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17a51efb-d97f-44a6-9584-70fdc41b63b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [{'question': 'Did Sean Connery write a letter...\n",
       "1      [{'question': 'Were documents related to this ...\n",
       "2      [{'question': 'Did Imran Khan criticize Macron...\n",
       "4      [{'question': 'Is Matt Gaetz a Republican?', '...\n",
       "5      [{'question': 'Do the articles published by th...\n",
       "                             ...                        \n",
       "221    [{'question': 'Did Nigeria gain independence?'...\n",
       "222    [Does Nigeria currently grapple with multiple ...\n",
       "223    [{'question': 'What are Nigeria's petrol price...\n",
       "224    [{'question': 'Did Nigeria gain independence?'...\n",
       "225    [{'question': 'Was there a drop in oil prices ...\n",
       "Name: evidence, Length: 215, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['evidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cd15c62-1ac3-493c-aca5-2c54d343e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_q_dict(x):\n",
    "    if type(x[0]) is dict:\n",
    "        return x\n",
    "    elif type(x[0]) is str:\n",
    "        res = []\n",
    "        for qa in x:\n",
    "            res.append({'question':qa.split(\"\\t\")[0],\n",
    "             'answer':qa.split(\"\\t\")[-1]})\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac9e73fc-4e3d-48a8-b0a0-ded5dbd063bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.dropna(subset=['evidence'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "080a4679-4b1d-4769-b094-3dec7f53ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['evidence'] = eval_df['evidence'].apply(lambda x: fix_q_dict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f4290-f407-42fa-a656-7dbcc99eed73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "576b55b4-5a50-479c-b6af-97f729505193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_samples = []\n",
    "for i, sample in  eval_df.iterrows():\n",
    "    claim = sample['claim']\n",
    "    label = sample['pred_label']\n",
    "    prediction_evidence = \"\"\n",
    "    #print (len(sample['evidence']))\n",
    "    for src_qa in sample['evidence']:    \n",
    "        # print (sample['evidence'])\n",
    "        prediction_evidence += src_qa[\"question\"] + \"\\t\\t\\n\" + src_qa[\"answer\"] + \"\\t\\t\\n\\n\"\n",
    "    #\n",
    "    # print (\"Claim:\\n\", claim)\n",
    "    # print (prediction_evidence)\n",
    "    new_samples.append([i, claim, prediction_evidence, label, 'pred'])\n",
    "\n",
    "with open(\"leaderboard_submission/submission.csv\".format(\"eyigoz\"), mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"claim\", \"evi\", \"label\", \"split\"])  # Write header\n",
    "    writer.writerows(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "815df39c-1b28-4b80-b854-d1b4dedf3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['claim_id'] = eval_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "505878df-5232-42b8-90bf-0acc94c30791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, 22)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval_df['claim_id']\n",
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa4590ce-dfb4-47f0-a096-6b97bd69e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[['claim_id','claim','evidence','pred_label']].reset_index(drop=True).to_json('train_predictions.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b338098-4ec5-41e4-98a4-5276e50ae422",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[['claim_id','claim','questions','claim_types','label']].reset_index(drop=True).to_json('train_gold.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e296fa77-80c8-4c04-a48b-03ac6454fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Event/Property Claim']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.iloc[0]['claim_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8da417ca-2b09-462d-8dd5-238094f0b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_samples = []\n",
    "# for i, sample in eval_df.iterrows():\n",
    "#     claim = sample['claim']\n",
    "#     label = sample['pred_label']\n",
    "#     prediction_evidence = \"\"\n",
    "#     for src_qa in sample['questions']:\n",
    "#         prediction_evidence += src_qa[\"question\"] + \"\\t\\t\\n\" + src_qa[\"answer\"] + \"\\t\\t\\n\\n\"\n",
    "#     #\n",
    "#     new_samples.append([i, claim, prediction_evidence, label, 'pred'])\n",
    "\n",
    "# with open(\"leaderboard_submission/gold.csv\".format(\"eyigoz\"), mode=\"w\", newline=\"\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"id\", \"claim\", \"evi\", \"label\", \"split\"])  # Write header\n",
    "#     writer.writerows(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d52f09-8e77-4573-baf3-545abd8d662e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf00bae-179e-456d-82dc-2cd7edd8a020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85c98d-f85b-4aaf-9646-7ea21690e532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
