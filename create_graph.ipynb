{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9a9423-dcec-40b9-9820-73387cd88dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315a8d1abd944e888591cb7a058a50ae\n"
     ]
    }
   ],
   "source": [
    "# Load API keys securely\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"RITS_API_KEY\"] = os.getenv(\"RITS_API_KEY\")\n",
    "print (os.environ[\"RITS_API_KEY\"])\n",
    "# Core libraries\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import TypedDict, Annotated, List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "# NLP + date parsing\n",
    "import dateparser\n",
    "import tiktoken\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# HTML parsing\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "# Text preprocessing\n",
    "import swifter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain: modular imports (latest versions)\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langgraph.graph import StateGraph, END\n",
    "from serpapi import GoogleSearch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61388e7-48e8-4188-9bf2-d135200356cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7152507b-d67d-4123-8b1b-f87d6db46022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# echo 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 > .huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2787d-63b6-44d5-853d-395de1b20e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face token here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024bfa33-d0e8-4832-a882-d977547d098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbaa65-dfd4-42d6-8530-5e5c3ebd7f9d",
   "metadata": {},
   "source": [
    "# Hi this is my text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e455af30-1f95-47fb-82b8-a66224fadaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting models\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m model_urls \u001b[38;5;241m=\u001b[39m get_rits_model_list()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# url = 'https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/deepseek-v3'\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m url \u001b[38;5;241m=\u001b[39m model_urls[\u001b[43mmodel_id\u001b[49m]\n\u001b[1;32m     27\u001b[0m url\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "rits_api_key = os.environ[\"RITS_API_KEY\"]\n",
    "TIMEOUT = 40\n",
    "\n",
    "model_name = 'deepseek-ai/DeepSeek-V3'\n",
    "# find the map of huggingface model name\n",
    "def get_rits_model_list():\n",
    "    url = \"https://rits.fmaas.res.ibm.com/ritsapi/inferenceinfo\"\n",
    "    response = requests.get(url, headers={\"RITS_API_KEY\": rits_api_key})\n",
    "    if response.status_code == 200:\n",
    "        return {m[\"model_name\"]: m[\"endpoint\"] for m in response.json()}\n",
    "    else:\n",
    "        raise Exception(f\"Failed getting RITS model list:\\n\\n{response.text}\")\n",
    "\n",
    "\n",
    "# basic model definitions\n",
    "print(\"getting models\")\n",
    "# model_id = 'deepseek-ai/DeepSeek-V3'\n",
    "# model_id='meta-llama/llama-4-scout-17b-16e'\n",
    "# #model_id='meta-llama/llama-3-3-70b-instruct-embeddings'\n",
    "# model_id=\"ibm-granite/granite-3.1-8b-instruct\"\n",
    "# model_id='mistralai/mixtral-8x22B-instruct-v0.1'\n",
    "\n",
    "model_urls = get_rits_model_list()\n",
    "\n",
    "# url = 'https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/deepseek-v3'\n",
    "url = model_urls[model_id]\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd033376-295e-4331-816d-6a63205c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer#, LlamaTokenizer\n",
    "\n",
    "encoding = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x22B-Instruct-v0.1\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde584dc-928c-4806-95fc-ba20c02c8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Define the state schema ===\n",
    "\n",
    "class State(TypedDict):\n",
    "    text: str\n",
    "    topic: str\n",
    "    questions: Annotated[list[str], operator.add]\n",
    "    claim: str\n",
    "    date: str\n",
    "    search_query: str    \n",
    "    search_results: List[str]\n",
    "    summaries: str\n",
    "    num_sentences: int\n",
    "    relevant_sentences: List[str]\n",
    "    verifier_result: str\n",
    "    generated_evidence: List[str]\n",
    "    qa_pairs: str\n",
    "    \n",
    "# === Initialize the language model ===\n",
    "# === LLM setup ===\n",
    "llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "\n",
    "# === Define the fixed list of topics ===\n",
    "\n",
    "# === Classifier chain ===\n",
    "classifier_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"topic_list\"],\n",
    "    template=(\n",
    "        \"You are a topic classifier. Given the text below, classify it \"\n",
    "        \"into one of these topics: {topic_list}.\\n\\n\"\n",
    "        \"Put the result in **, as in **result**\"\n",
    "        \"Text: \\\"{text}\\\"\\n\"\n",
    "        \"Topic:\"\n",
    "    ),\n",
    ")\n",
    "classifier_chain = classifier_prompt | llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793e4e9-4f32-4027-ae5e-d44772c6fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analyzer chains: break claim into verifiable questions ===\n",
    "def make_question_generator_chain(topic):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=(\n",
    "            f\"You are an expert in {topic}. Given the following claim, break it down into individual verifiable questions. \"\n",
    "            f\"Every phrase or assertion should become a question that can be fact-checked.\\n\\n\"\n",
    "            f\"Do not generate questions that inquire about information that is not mentioned in the question.\\n\\n\"\n",
    "            f\"Do not generate questions that cannot be used to verify or refute the claim.\\n\\n\"\n",
    "            f\"Claim: \\\"{{text}}\\\"\\n\\n\"\n",
    "            f\"Questions:\"\n",
    "        )\n",
    "    )\n",
    "    return prompt | llm  \n",
    "\n",
    "analyzer_chains = {topic: make_question_generator_chain(topic) for topic in topics}\n",
    "\n",
    "# === Define LangGraph nodes ===\n",
    "def classifier_node(state: State) -> dict:\n",
    "    text = state['text']\n",
    "    result = classifier_chain.invoke({\"text\": text, \"topic_list\": \", \".join(topics)})\n",
    "    result = result.content.strip()\n",
    "    results = re.findall(r'\\*\\*(.*)\\*\\*', result)\n",
    "    if results:\n",
    "        topic = results[-1]\n",
    "    else:\n",
    "        topic = 'Unspecified topic'\n",
    "    \n",
    "    return {\"topic\": topic}\n",
    "\n",
    "def analyzer_node(state: State) -> dict:\n",
    "    topic = state['topic']\n",
    "    text = state['text']\n",
    "    analyzer_chain = analyzer_chains.get(topic)\n",
    "    if analyzer_chain:\n",
    "        questions_output = analyzer_chain.invoke({\"text\": text}).content.strip() \n",
    "        questions = [q.strip() for q in questions_output.split('\\n') if q.strip()]\n",
    "        return {\"questions\": questions}\n",
    "    else:\n",
    "        return {\"questions\": [f\"No analyzer found for topic: {topic}\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed452e6-76e9-4afa-a825-13d89cf2acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(x):\n",
    "    try:\n",
    "        return dateparser.parse(str(x)).strftime(\"%m/%d/%Y\")\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea3a39-e960-47f1-9c26-476ce2f173ec",
   "metadata": {},
   "source": [
    "# Obtain evidence from web "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e233d-8ee7-47fd-a880-3e5fb88acb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946ad8c-bdb0-441b-ad6d-a94161a9fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".mp4\", \".mov\", \".avi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42a337-dda4-4f88-bfb3-538515892466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_url):\n",
    "    response = requests.get(pdf_url, timeout=(3,10))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        # print(\"Content-Type:\", content_type)\n",
    "        # print(\"Content-Length:\", len(response.content))\n",
    "        \n",
    "        # if 'pdf' not in content_type.lower():\n",
    "        #     print(\"Warning: This does not look like a PDF file!\")\n",
    "        pdf_data = response.content    \n",
    "        doc = fitz.open(stream=pdf_data, filetype=\"pdf\")\n",
    "        \n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "        \n",
    "        return full_text\n",
    "    #else:\n",
    "        \n",
    "        #print(f\"Failed to download. Status code: {response.status_code}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbf0fd-3f3b-4508-a798-0b2becbc985d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca0a44-fb8b-423b-a17d-7a3f00c113bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the search tool using SerpAPI\n",
    "def process_results(res):\n",
    "    title = res.get(\"title\", \"\")\n",
    "    link = res.get(\"link\", \"\")\n",
    "    #print (link)\n",
    "    if not (\"Just a moment\" in title or \"Enable JavaScript\" in title):\n",
    "        try:\n",
    "            if res['link'].endswith(\".pdf\"):\n",
    "                text = extract_pdf_text(res['link'])\n",
    "                #print (text[:50])\n",
    "                search_result_2 = re.sub(r'\\n\\s*\\n','\\n\\n',text.strip())\n",
    "                search_result_3 = re.sub(r'\\n\\s*\\w+(\\s+\\w+)?\\s*\\n','',search_result_2)\n",
    "                return search_result_3\n",
    "\n",
    "            elif not (any(res['link'].endswith(ext) for ext in media_extensions)):                        \n",
    "                response = requests.get(res['link'],timeout=(3, 10))\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Get all text from the page\n",
    "                text = soup.get_text()\n",
    "                #print (text[:50])\n",
    "                search_result_2 = re.sub(r'\\n\\s*\\n','\\n\\n',text.strip())\n",
    "                search_result_3 = re.sub(r'\\n\\s*\\w+(\\s+\\w+)?\\s*\\n','',search_result_2)\n",
    "                return search_result_3\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            print (res['link'])\n",
    "            return ''\n",
    "def search_query(query,from_str):\n",
    "    if verbose:\n",
    "        print ('query', query)\n",
    "    params = {\n",
    "      \"q\": query,       # Your search query\n",
    "      \"hl\": \"en\",                       # Language\n",
    "      \"gl\": \"us\",                       # Region\n",
    "      \"api_key\": \"710dff5ac2a065d1016d49de2018adce28bd01a913e40279d8bd9a059c9923f6\",   # Replace with your actual API key\n",
    "       \"num\": max_hits_per_query,\n",
    "      \"tbs\": f\"cdr:1,cd_min:{from_str},cd_max:12/31/2099\",\n",
    "      'engine':search_engine\n",
    "    }\n",
    "    # print (params,query)\n",
    "    search = GoogleSearch(params)\n",
    "    all_results = []\n",
    "\n",
    "    try:\n",
    "        results = search.get_dict()\n",
    "        # Print top organic results\n",
    "        #print (results)\n",
    "        organic_results =  results.get(\"organic_results\", [])\n",
    "        \n",
    "        # Send a GET request\n",
    "        if verbose:\n",
    "            print (\"len results\", len(organic_results))\n",
    "\n",
    "        if len(organic_results) > 0:\n",
    "            pool = Pool(processes=min(len(organic_results),max_hits_per_query))\n",
    "            all_results = pool.map(process_results,organic_results)\n",
    "                #print (res)\n",
    "    except Exception as e:\n",
    "        #print (e)\n",
    "        pass\n",
    "        \n",
    "                \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522a723-b829-4aba-b564-43ffb02f4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === State definition ===\n",
    "\n",
    "\n",
    "# === LLM setup ===\n",
    "first_attempt_llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0.0,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "second_attempt_llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0.1,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "# === Step 1: Query Builder ===\n",
    "query_first_attempt_prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a search query expert. Given the claim:\\n\\\"{claim}\\\"\\n\"\n",
    "        \"generate a Google search query that will help find evidence to verify or refute this claim. \\n\"\n",
    "        \"don't put quotes on the entire query, we are not looking for exact match for the query.\\n\"\n",
    "        \"Don't use everything if the claim is long, summarize it.\\n\"\n",
    "        \"put commas between the keyphrases you generated\"\n",
    "        \"Return a json with only one field 'query', no explanation in the output\\n\"\n",
    "        \"\\n\\nJSON:\"\n",
    "    )\n",
    ")\n",
    "query_second_attempt_prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a search query expert. Given the claim:\\n\\\"{claim}\\\"\\n\"\n",
    "        \"generate a Google search query that will help find evidence to verify or refute this claim. \\n\"\n",
    "        \"don't put quotes on the entire query, we are not looking for exact match for the query.\\n\"\n",
    "        \"Don't use everything if the claim is long, summarize it.\\n\"\n",
    "        \"The query must be at most 7 tokens long.\\n\"\n",
    "        \"put commas between the keyphrases you generated\"\n",
    "        \"do not generate this query: \\\"{search_query}\\\"\"\n",
    "        \"Return a json with only one field 'query', no explanation in the output\\n\"\n",
    "        \"\\n\\nJSON:\"\n",
    "    )\n",
    ")\n",
    "query_builder_first_attempt_chain = query_first_attempt_prompt | first_attempt_llm\n",
    "query_builder_second_attempt_chain = query_second_attempt_prompt | second_attempt_llm\n",
    "\n",
    "def query_builder_node(state: State) -> dict:\n",
    "    \n",
    "    state['questions'] = set(state['questions'])\n",
    "    query = ''\n",
    "    #print (\"In query builder\\n\", state)\n",
    "    if state['search_query'] == '':\n",
    "        result  = query_builder_first_attempt_chain.invoke({\n",
    "            \"claim\": state['claim'],\n",
    "            'search_query':state['search_query']\n",
    "        })\n",
    "    else:\n",
    "        result  = query_builder_second_attempt_chain.invoke({\n",
    "            \"claim\": state['claim'],\n",
    "            'search_query':state['search_query']\n",
    "        })    \n",
    "    try:\n",
    "        query = result.content.strip()\n",
    "        query = query.strip('`').replace(\"json\\n\", \"\", 1).strip()\n",
    "        query = re.match(r'.*(\\{.+\\}).*', query,re.DOTALL).group(1)\n",
    "        query = re.match(r'\\{.*?\\:(.*)\\}', query,re.DOTALL).group(1)\n",
    "        # parsed = json.loads(query)\n",
    "        # query = parsed['query']\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (query)\n",
    "        pass\n",
    "    return {\"search_query\": query}\n",
    "\n",
    "# === Step 2: Search Execution (SerpAPI) ===\n",
    "#from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "#search_tool = SerpAPIWrapper()\n",
    "\n",
    "def search_node(state: State) -> dict:\n",
    "    state['questions'] = set(state['questions'])\n",
    "    query = state['search_query']\n",
    "    from_str = parse_dates(state['date'])\n",
    "    results = search_query(query,from_str)  \n",
    "    return {\"search_results\": results}\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157d5dd-7983-4876-ad9c-70fe6b3a8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_chunks_to_summarize(long_text) : \n",
    "    # Initialize tokenizer (for GPT-4o)\n",
    "    \n",
    "    # Helper: count tokens in text\n",
    "    def count_tokens(text):\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    # Define safe token window (adjust for your summarizer)\n",
    " # safe per summarizer call\n",
    "     # overlap for coherence\n",
    "    \n",
    "    # Convert token targets to rough character sizes (~4 chars/token conservative)\n",
    "    chunk_size_chars = max_chunk_tokens * 4\n",
    "    chunk_overlap_chars = desired_overlap_tokens * 4\n",
    "    \n",
    "    # Initialize recursive splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size_chars,\n",
    "        chunk_overlap=chunk_overlap_chars\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Step 1: Character-based split\n",
    "        char_chunks = splitter.split_text(long_text)\n",
    "        \n",
    "        # Step 2: Validate and trim by token count (if needed)\n",
    "        final_chunks = []\n",
    "        for chunk in char_chunks:\n",
    "            token_count = count_tokens(chunk)\n",
    "            if token_count <= max_chunk_tokens:\n",
    "                final_chunks.append(chunk)\n",
    "            else:\n",
    "                #print(f\"Warning: Oversized chunk ({token_count} tokens), trimming...\")\n",
    "                # Optionally trim further by tokens\n",
    "                tokens = encoding.encode(chunk)\n",
    "                trimmed = encoding.decode(tokens[:max_chunk_tokens])\n",
    "                final_chunks.append(trimmed)\n",
    "        \n",
    "        # Report\n",
    "        # print(f\"Prepared {len(final_chunks)} chunks for summarization.\")\n",
    "        total_tokens = sum([count_tokens(c) for i, c in enumerate(final_chunks)])\n",
    "        return final_chunks, total_tokens\n",
    "    except Exception as e:\n",
    "        # print (e)\n",
    "        return [],0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Step 3: Summarize node ===\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"claim\", \"chunk\", \"num_tokens\", 'questions'],\n",
    "    template=(\n",
    "        \"You are an summarizer. When you summairze keep this in mind: \\n\"\n",
    "        \"Given a claim and questions about the claim \"\n",
    "        \"there are web search results in which we will look for evidence to \"\n",
    "        \"refute or verify the claim through finding answer to the questions. \"\n",
    "        \"The results can be very long, so they have to be summarized. \"\n",
    "        \"So, given this claim:\\n\\\"{claim}\\\"\\n\"\n",
    "        \"and the following questions about the claim {questions}\\n, go through this search result:\\n\"\n",
    "        \"{chunk}\\\"\\n\"\n",
    "        \"summarize the result to {num_tokens}.\"\n",
    "        \"Do not try to verify of refute the claim, only summarize it without dropping information that\"\n",
    "        \" can be used to answer these questions.\\n\"\n",
    "        \"If document is corrupted and cannot be summaried, return empty string. \"\n",
    "        \"\\n\\nSummary:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def summarize_chunks(chunks,percent_reduce,result,state):\n",
    "    try:\n",
    "        if len(chunks) == 0:\n",
    "            return \"\"\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_id,\n",
    "            temperature=0,\n",
    "            api_key=\"/\",\n",
    "            base_url=f'{url}/v1',\n",
    "            default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "            timeout=TIMEOUT\n",
    "        )\n",
    "        if percent_reduce == 1:\n",
    "            return result\n",
    "\n",
    "        time.sleep(5/random.randint(1, 10))\n",
    "        summarizer_chain = summarize_prompt | llm\n",
    "        summaries = []\n",
    "        percent_reduce = min(percent_reduce,1)\n",
    "        #print (\"len chunks to summarize\", len(chunks))\n",
    "        for chunk in chunks:\n",
    "            len_tokens  = len(encoding.encode(chunk))\n",
    "            num_tokens = min(max(int(len_tokens * percent_reduce),10),len_tokens)\n",
    "            #print (\"num_tokens\", num_tokens, \"len_tokens\", len_tokens, \"percent_reduce\", percent_reduce)\n",
    "            \n",
    "            result = summarizer_chain.invoke({\n",
    "            \"claim\": state['claim'],\n",
    "            \"chunk\": chunk,\n",
    "            \"num_tokens\": num_tokens ,\n",
    "            \"questions\": \"\\n\".join(set(state['questions'])) # Pass as string if needed\n",
    "            })\n",
    "            summary = result.content.strip()\n",
    "            summaries.append(summary)\n",
    "        return \"\\n\".join(summaries)   \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return ''\n",
    "\n",
    "def summarizer_node(state: State) -> dict:\n",
    "    state['questions'] = set(state['questions'])\n",
    "    results = state['search_results']\n",
    "    all_chunks = []\n",
    "    all_total_tokens = 0 \n",
    "    percent_reduce = np.inf\n",
    "    token_count = []\n",
    "    for result in results:\n",
    "        chunks,total_tokens = get_chunks_to_summarize(result)\n",
    "        if total_tokens > 0:\n",
    "            all_total_tokens += total_tokens\n",
    "            token_count.append(total_tokens)\n",
    "            all_chunks.append(chunks)   \n",
    "            if all_total_tokens > 0:\n",
    "                percent_reduce = max_token_count / all_total_tokens\n",
    "            if percent_reduce < 0.1:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        #print (\"len(all_chunks) \", len(all_chunks), \"max_chunk_tokens \", \\\n",
    "        #   max_chunk_tokens, \"percent_reduce\" , percent_reduce)\n",
    "    token_count =  np.array(token_count)\n",
    "    token_percentages_per_result = np.array(token_count) / all_total_tokens\n",
    "\n",
    "#     import numpy as np\n",
    "\n",
    "# token_count = np.array([800, 1200, 500, 1500])  # example token counts per result\n",
    "# all_total_tokens = token_count.sum()\n",
    "# max_token_count = 1000  # your allowed total after reduction\n",
    "    if all_total_tokens == 0:\n",
    "        return {\"summaries\": \"\", 'num_sentences':0}\n",
    "    # Step 1: Compute relative weights â€” more aggressive for longer chunks\n",
    "    if verbose:\n",
    "        print (\"token_count\", token_count)\n",
    "    #print (\"token_percentages_per_result\",token_percentages_per_result)    \n",
    "    inverse_lengths = 1 / np.emath.logn(max_chunk_tokens, token_count)\n",
    "    weights = inverse_lengths / inverse_lengths.sum()\n",
    "    #print (\"weights\",weights)\n",
    "    target_tokens_per_result = weights * max_token_count\n",
    "    # Step 2: Compute how many tokens each chunk gets (out of max_token_count)\n",
    "    #print (\"target_tokens_per_result\",target_tokens_per_result)\n",
    "    #print (\"token_count\",token_count)\n",
    "    target_tokens_per_result =  np.min(np.vstack((target_tokens_per_result,token_count)),axis=0)\n",
    "    #print (\"target_tokens_per_result\",target_tokens_per_result)\n",
    "    leftout_tokens = max_token_count - target_tokens_per_result.sum()\n",
    "    #print (\"leftout_tokens\",leftout_tokens)\n",
    "    to_add_tokens = np.maximum(np.subtract(token_count,target_tokens_per_result),0)\n",
    "    if to_add_tokens.sum() > 0:\n",
    "        to_add_tokens = (to_add_tokens / to_add_tokens.sum()) * leftout_tokens\n",
    "    #print (\"to_add_tokens\",to_add_tokens)\n",
    "    target_tokens_per_result = (target_tokens_per_result + to_add_tokens).astype(int)\n",
    "    if verbose:\n",
    "        print (\"target_tokens_per_result\",target_tokens_per_result)\n",
    "\n",
    "        \n",
    "    #print (\"target_tokens_per_result\",target_tokens_per_result)\n",
    "    # Step 3: Convert that to a reduction ratio (per result)\n",
    "    percent_reduce_per_result = target_tokens_per_result / token_count\n",
    "    #print (\"percent_reduce_per_result\",percent_reduce_per_result)\n",
    "    # Optional: clip between [0.05, 1.0] to avoid extreme reductions\n",
    "    percent_reduce_per_result = np.clip(percent_reduce_per_result, 0.01, 1.0)\n",
    "    #print (\"percent_reduce_per_result\",percent_reduce_per_result)\n",
    "\n",
    "    \n",
    "    all_summaries = []\n",
    "    if percent_reduce_per_result.min() < 1 and len(all_chunks) > 0:\n",
    "        pool = Pool(processes=num_processes)\n",
    "        all_summaries = pool.starmap(summarize_chunks,[(c,p,r,state) \\\n",
    "                                                       for c,p,r in zip(all_chunks,percent_reduce_per_result,results)])\n",
    "        all_summaries = \"\\n\".join(all_summaries)\n",
    "    else:\n",
    "        all_summaries = \"\\n\".join([\" \".join(c) for c in all_chunks])\n",
    "    num_tokens = len(encoding.encode(all_summaries))\n",
    "    num_sentences = int(min(num_tokens / max_token_count,1) * 100)\n",
    "    #print (\"num_sentences\",num_sentences, \"num_tokens\",num_tokens,\"MAX_OVERALL_TOKENS\",MAX_OVERALL_TOKENS)\n",
    "    return {\"summaries\": all_summaries, 'num_sentences':num_sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d925949-40cc-4a1a-bfd6-d1bf87557a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Relevant Sentence Extractor ===\n",
    "extractor_prompt = PromptTemplate(\n",
    "    input_variables=[\"claim\", \"summaries\", \"num_sentences\" 'questions'],\n",
    "    template=(\n",
    "        \"You are an evidence extractor. Given the claim:\\n\\\"{claim}\\\"\\n\"\n",
    "        \"and the following questions about the claim \\n\\\"{questions}\\\"\\n, go through these search results:\\n\\\"{summaries}\\\"\\n\"\n",
    "        \"extract the most relevant sentences that directly address or provide evidence about the claim.\"\n",
    "        \"extract at most {num_sentences} sentences, you don't have to extract anything if there is nothing relevant\"\n",
    "        \"sort sentences in terms of relenavce so that most relevant comes first, and least comes last\"\n",
    "        \"\\n\\nRelevant Sentences:\"\n",
    "    )\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "extractor_chain = extractor_prompt | llm\n",
    "\n",
    "def extractor_node(state: State) -> dict:\n",
    "    state['questions'] = set(state['questions'])\n",
    "    result = extractor_chain.invoke({\n",
    "        \"claim\": state['claim'],\n",
    "        \"summaries\": str(state['summaries']),\n",
    "        \"num_sentences\": state['num_sentences'] ,\n",
    "        \"questions\": \"\\n\".join(state['questions']) # Pass as string if needed\n",
    "    })\n",
    "    sentences = result.content.strip().split('\\n')\n",
    "    return {\"relevant_sentences\": [s.strip() for s in sentences if s.strip()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc889c-718a-4920-abd7-a19242fc95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Relevant Sentence Extractor ===\n",
    "evidence_gen_prompt = PromptTemplate(\n",
    "    input_variables=[\"claim\", 'questions'],\n",
    "    template=(\n",
    "        \"You are an evidence generator. Given the claim:\\n\\\"{claim}\\\"\\n\"\n",
    "        \"and the following questions about the claim {questions}\\n\"\n",
    "        \"you will search your knowldge base for statements that can be used to verify or refute the claim.\\n\"\n",
    "        \"Only provide answers to the questions. If you don't have an aswer in your knowledge base, you don't have to generate an answer.\\n\"\n",
    "        \"Only generate sentences that you are very strongly confident are true.\\n\"\n",
    "        \"If there is no evidence in your knowledge base that strongly supports or refutes the claim, return \\\"NONE\\\". \\n\"\n",
    "        \"Don't forget, lack of evidence does not mean the claim is false or refuted. \\n\"\n",
    "        \"\\nEvidence:\"\n",
    "    )\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "evidence_gen_chain = evidence_gen_prompt | llm\n",
    "\n",
    "def evidence_gen_node(state: State) -> dict:\n",
    "    state['questions'] = set(state['questions'])\n",
    "    result = evidence_gen_chain.invoke({\n",
    "        \"claim\": state['claim'],\n",
    "        \"questions\": \"\\n\".join(state['questions']) # Pass as string if needed\n",
    "    })\n",
    "    sentences = result.content.strip().split('\\n')\n",
    "    return {\"generated_evidence\": [s.strip() for s in sentences if s.strip()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba4367-2a08-4c5e-bb15-a87b5b78c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Relevant Sentence Extractor ===\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"questions\",'relevant_sentences', \"generated_evidence\",],\n",
    "    template=(\n",
    "        \"Given a list of questions and relevant sentences,\\n\"\n",
    "        \"you will find a mapping between questions and sentences, and generate question-answer pairs. \\n\"\n",
    "        \"You will provide an answer to each question using the evidence provided. \\n\"\n",
    "        \"Do not generate answers that are not provided in the evidence. Use only the provided evidence\\n\"\n",
    "        \"if a question cannot be answered using the provided evidence, the answer should be \\\"No answer found\\\". \\n\"\n",
    "        \"if a question has multiple answers in the given list, pick only the most relevant answer. \\n\"\n",
    "        \"Split the question and its answer by three tabs. For example:\\n\"\n",
    "        \"\\\"Did Obama die in 2018?\\t\\t\\tObama gave a speech in 2021, therefore he was still alive in 2018.\\\" \\n\"\n",
    "        \"Here are the questions: \\n{questions}\\n\"\n",
    "        \"Here is the evidence :  \\n{relevant_sentences}, {generated_evidence}\\n\"\n",
    "        \"Put your result in a json, NOT prose\\n\"\n",
    "        \"JSON: \"\n",
    "    )\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "def qa_node(state: State) -> dict:\n",
    "    state['questions'] = set(state['questions'])\n",
    "    result = qa_chain.invoke({\n",
    "        \"questions\": \"\\n\".join(state['questions']), # Pass as string if needed\n",
    "        \"relevant_sentences\": \"\\n\".join(state['relevant_sentences']),\n",
    "        \"generated_evidence\": \"\\n\".join(state['generated_evidence'])\n",
    "    })\n",
    "    result = result.content.strip()\n",
    "    result = result.strip('`').replace(\"json\\n\", \"\", 1).strip()\n",
    "    return {\"qa_pairs\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637abde-6577-4250-b2b5-72233d0817d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Relevant Sentence Extractor ===\n",
    "verifier_prompt = PromptTemplate(\n",
    "    input_variables=[\"claim\", \"qa_pairs\"],\n",
    "    template=(\n",
    "        \"You are a claim verifier. Given the claim:\\n\\\"{claim}\\\"\\n\"\n",
    "        \"and the following question answer pairs seperated by tabs \\n\\\"{qa_pairs}\\\"\\n\\n\"\n",
    "        \"Classify whether the evidence supports the claim, whether it refutes the claim, whethere there is not enough evidence for either, or there is conflicting evidence.\\n\"\n",
    "        \" A single contradiction is enough to refute the claim, while full support requires consistent verification of all sub-parts. \"\n",
    "        \"If the same questions has conflicting answers, then there is conflicting evidence.\\n\"\n",
    "        \"Result in JSON with fields result and explanation.\\n\"\n",
    "        \"The result will be either one of these tokens = supports, refutes, not_enough_evidence, conflicting_evidence.\\n\"      \n",
    "        \"JSON: \"\n",
    "    )\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=model_id,\n",
    "    temperature=0.0,\n",
    "    api_key=\"/\",\n",
    "    base_url=f'{url}/v1',\n",
    "    default_headers={'RITS_API_KEY': os.getenv(\"RITS_API_KEY\")},\n",
    "    timeout=TIMEOUT\n",
    ")\n",
    "\n",
    "verifier_chain = verifier_prompt | llm\n",
    "\n",
    "def verifier_node(state: State) -> dict:\n",
    "    state['questions'] = set(state['questions'])\n",
    "    result = verifier_chain.invoke({\n",
    "        \"claim\": state['claim'],\n",
    "        \"qa_pairs\":state['qa_pairs']\n",
    "    })\n",
    "    result = result.content.strip()\n",
    "    return {\"verifier_result\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be627c6-e8d0-47db-b108-d8958b558563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_loop_router(state: State) -> dict:\n",
    "    if state['search_results'] == []:\n",
    "        return \"search_again\"\n",
    "    else:\n",
    "        return \"summarize\"\n",
    "def summarize_loop_router(state: State) -> dict:\n",
    "    if state['summaries'] == \"\":\n",
    "        return \"search_again\"\n",
    "    else:\n",
    "        return \"extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65d1b2-21c5-4982-8faa-b9839f9909c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"classifier\", classifier_node)\n",
    "graph_builder.add_node(\"analyzer\", analyzer_node)\n",
    "graph_builder.add_node(\"query_builder\", query_builder_node)\n",
    "graph_builder.add_node(\"search\", search_node)\n",
    "graph_builder.add_node(\"search_loop_node\", lambda x: x)\n",
    "graph_builder.add_node(\"summarize_loop_node\", lambda x: x)\n",
    "\n",
    "graph_builder.add_node(\"summarizer\", summarizer_node)\n",
    "#graph_builder.add_node(\"evidence_gen\", evidence_gen_node)\n",
    "graph_builder.add_node(\"extractor\", extractor_node)\n",
    "graph_builder.add_node(\"qa\", qa_node)\n",
    "graph_builder.add_node(\"verifier\", verifier_node)\n",
    "\n",
    "graph_builder.add_edge(\"classifier\", \"analyzer\")\n",
    "graph_builder.add_edge(\"analyzer\", \"query_builder\")\n",
    "graph_builder.add_edge(\"query_builder\", \"search\")\n",
    "graph_builder.add_edge(\"search\", \"search_loop_node\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"search_loop_node\",   # node name (as string)\n",
    "    search_loop_router,   # router function\n",
    "    {\n",
    "        \"search_again\": \"query_builder\",\n",
    "        \"summarize\": \"summarizer\",\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(\"summarizer\", \"summarize_loop_node\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"summarize_loop_node\",   # node name (as string)\n",
    "    summarize_loop_router,   # router function\n",
    "    {\n",
    "        \"search_again\": \"query_builder\",\n",
    "        \"extract\": \"extractor\",\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"extractor\",'qa')\n",
    "#graph_builder.add_edge(\"evidence_gen\",'qa')\n",
    "graph_builder.add_edge(\"qa\",'verifier')\n",
    "\n",
    "# graph_builder.set_finish_point(\"search\")\n",
    "#graph_builder.set_finish_point(\"summarizer\")\n",
    "graph_builder.set_entry_point(\"classifier\")\n",
    "graph_builder.set_finish_point(\"verifier\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125c0d2-e18a-41c6-8d16-316965d960b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba546d1-aa7a-4c02-b9b3-09432eea6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(row):\n",
    "    #print (row)\n",
    "    initial_state = {\n",
    "        \"text\" : row['claim'],\n",
    "        \"claim\": row['claim'],\n",
    "        \"topic\":\"\",\n",
    "        \"date\": row['claim_date'],\n",
    "        \"questions\": [],#re.search(r'questions(.*)$',row['topics_questions']).group(1).strip('{}[]'),\n",
    "        'search_query': '',\n",
    "        \"search_results\": [],\n",
    "        \"num_sentences\": 0,\n",
    "        \"summaries\": \"\",\n",
    "        \"relevant_sentences\": [],\n",
    "        \"generated_evidence\": [],\n",
    "        \"verifier_result\":''\n",
    "\n",
    "    }\n",
    "    return State(initial_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
